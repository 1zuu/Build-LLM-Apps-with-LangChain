{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat vs Completion Model\n",
    "\n",
    "In the context of OpenAI's GPT-3.5 architecture \"\"chat\" and \"completion\" refer to two different ways of interacting with the model, and they serve different purposes:\n",
    "\n",
    "1. **Chat Model:**\n",
    "   - The \"chat\" format is designed for multi-turn conversations or dialogues.\n",
    "   - It is typically used when you want to have back-and-forth interactions with the model.\n",
    "   - You provide a series of messages as input, where each message has a role (\"system,\" \"user,\" or \"assistant\") and content (the text of the message). The conversation usually starts with a system message to set the context, followed by alternating user and assistant messages.\n",
    "   - The model processes the entire conversation history to generate a response, taking into account the context provided in previous messages.\n",
    "   - Chat models are useful for creating chatbots, virtual assistants, or simulating conversation with a knowledgeable entity.\n",
    "   \n",
    "\n",
    "2. **Completion Model:**\n",
    "   - The \"completion\" format is designed for single-turn tasks or generating text based on a prompt.\n",
    "   - It is typically used for tasks like language translation, text summarization, text generation, and more.\n",
    "   - You provide a single prompt as input, and the model generates a completion or response based on that prompt.\n",
    "   - Completion models are useful for generating text that doesn't require ongoing interaction or conversation context.\n",
    "\n",
    "In summary, the main difference between the two lies in the format of input and the use case. \"Chat\" is used for multi-turn conversations, while \"completion\" is used for single-turn tasks or text generation based on a prompt. Keep in mind that OpenAI's models and offerings may have evolved since my last update in September 2021, so I recommend checking OpenAI's official documentation for the most up-to-date information on their offerings and capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, os, openai\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate, FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cadentials.yaml') as f:\n",
    "    credentials = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = credentials['OPENAI_API_KEY']\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = credentials['HUGGINGFACEHUB_API_TOKEN']\n",
    "os.environ['ENGINE'] = credentials['ENGINE']\n",
    "\n",
    "openai.api_key = credentials['OPENAI_API_KEY']\n",
    "openai.api_base = credentials['OPENAI_API_BASE']\n",
    "openai.api_type = credentials['OPENAI_API_TYPE']\n",
    "openai.api_version = credentials['OPENAI_API_VERSION']\n",
    "openai.engine = credentials['ENGINE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "                                        engine = os.environ[\"ENGINE\"],\n",
    "                                        model=\"gpt-3.5-turbo\",\n",
    "                                        messages=[\n",
    "                                                {\n",
    "                                                \"role\": \"system\", \n",
    "                                                \"content\": \"You are a helpful assistant.\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                \"role\": \"user\", \n",
    "                                                \"content\": \"Hello what kind of assistant are you?\"\n",
    "                                                },\n",
    "                                            ]\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-85GANtEaUrrfLHgp74qdDu6CWdVSo at 0x125ebcf90> JSON: {\n",
       "  \"id\": \"chatcmpl-85GANtEaUrrfLHgp74qdDu6CWdVSo\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1696264475,\n",
       "  \"model\": \"gpt-35-turbo-16k\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Hello! I am a helpful assistant designed to assist with a wide range of tasks and provide information and support. I can answer questions, provide recommendations, assist with organization and planning, and much more. How can I assist you today?\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 47,\n",
       "    \"prompt_tokens\": 25,\n",
       "    \"total_tokens\": 72\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Markup Language - token system\n",
    "\n",
    "```markdown\n",
    "<|im_start|>system\n",
    "You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible.\n",
    "Knowledge cutoff: 2021-09-01\n",
    "Current date: 2023-03-01<|im_end|>\n",
    "<|im_start|>user\n",
    "How are you<|im_end|>\n",
    "<|im_start|>assistant\n",
    "I am doing well!<|im_end|>\n",
    "<|im_start|>user\n",
    "How are you now?<|im_end|>\n",
    "```\n",
    "\n",
    "```\n",
    "import openai\n",
    "\n",
    "openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant named Kate.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello what kind of assistant are you?\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT: Hello! I'm Kate, your helpful assistant. How can I assist you today? \n",
      " 49 tokens used\n",
      "ChatGPT: I am a virtual assistant designed to help with a wide range of tasks and provide information on various topics. I can assist with answering questions, providing recommendations, giving directions, setting reminders, and much more. How can I assist you today? \n",
      " 112 tokens used\n",
      "ChatGPT: Yes, I have knowledge about football. Football, also known as soccer in some parts of the world, is a popular sport played by teams of 11 players each. It is played on a rectangular field where the objective is to score a goal by kicking the ball into the opposing team's net. The sport is governed by various organizations, such as FIFA (Fédération Internationale de Football Association), and has a rich history and culture surrounding it. If you have any specific questions about football, feel free to ask! \n",
      " 231 tokens used\n",
      "ChatGPT: As an AI assistant, I don't have personal opinions or biases. The debate between Barcelona and Real Madrid is a longstanding and passionate one among football fans. Both teams have their own rich histories and have achieved great success.\n",
      "\n",
      "Barcelona and Real Madrid are two of the most successful and popular football clubs in the world. Barcelona, based in Catalonia, Spain, is known for its possession-based playing style and has won numerous domestic and international titles. Real Madrid, based in Madrid, Spain, has a long history of success, known for its attacking style of play and has also won numerous domestic and international titles, including multiple UEFA Champions League titles.\n",
      "\n",
      "The \"best\" team is subjective and depends on individual preferences. Some may argue that Barcelona's technical style of play makes them the best, while others may prefer the exciting and attacking playstyle of Real Madrid. Ultimately, it comes down to personal preference and there isn't a definitive answer to which team is best. \n",
      " 440 tokens used\n",
      "ChatGPT: I apologize if my previous response wasn't clear. As an AI, I cannot provide a subjective opinion on which team is the best between Barcelona and Real Madrid. The determination of which team is the best is subjective and can vary depending on personal preferences, loyalty, and a multitude of other factors. Both Barcelona and Real Madrid have had periods of dominance and have achieved great success in domestic and international competitions. It ultimately boils down to individual opinions and preferences. \n",
      " 555 tokens used\n",
      "ChatGPT: As an AI assistant, I don't have personal opinions or biases. My purpose is to provide factual information and assist with various tasks. I can provide you with information about the achievements, history, and playing styles of Barcelona and Real Madrid, but I don't have the ability to form my own subjective opinion on which team is the best. \n",
      " 637 tokens used\n",
      "2024 tokens used in total in this conversation\n"
     ]
    }
   ],
   "source": [
    "conversation_total_tokens = 0\n",
    "\n",
    "while True:\n",
    "    message = input(\"Human: \")\n",
    "    if message=='exit':\n",
    "        print(f\"{conversation_total_tokens} tokens used in total in this conversation\")\n",
    "        break\n",
    "    if message:\n",
    "        messages.append(\n",
    "                        {\n",
    "                            \"role\": \"user\", \n",
    "                            \"content\": message\n",
    "                            },\n",
    "                        )\n",
    "        response = openai.ChatCompletion.create(\n",
    "                                                engine = os.environ[\"ENGINE\"],\n",
    "                                                model=\"gpt-3.5-turbo\", \n",
    "                                                messages=messages\n",
    "                                                )\n",
    "    \n",
    "    reply = response.choices[0].message.content\n",
    "    total_tokens = response.usage['total_tokens']\n",
    "    conversation_total_tokens += total_tokens\n",
    "    print(f\"ChatGPT: {reply} \\n {total_tokens} tokens used\")\n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI ChatGPT - Language Chain Integration\n",
    "### But why ?\n",
    "        - because we can change the output by employing control mechanisms via prompt template and can dynamically change the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_messages = [{\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"You are a helpful history professor named Zuu.\"\n",
    "                    }]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "                openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "                engine = os.environ[\"ENGINE\"],\n",
    "                model='gpt-3.5-turbo',\n",
    "                temperature=0.0, \n",
    "                max_tokens = 256\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_template = \"\"\"Take the following question: {user_input}\n",
    "\n",
    "# Answer it in an informative and intersting but conscise way for someone who is new to this topic.\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"Take the following question: {user_input}\n",
    "\n",
    "Answer should be very short\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "                        template=prompt_template, \n",
    "                        input_variables=[\"user_input\"]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### here we employed that the response should be \"informative and intersting but conscise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is subjective and depends on personal preference.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "user_input = \"Which is better ? barcelona or real madrid\"\n",
    "llm_chain.run(user_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
