{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, os\n",
    "from langchain import PromptTemplate\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms import CTransformers, HuggingFaceHub\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cadentials.yaml') as f:\n",
    "    credentials = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = credentials['HUGGINGFACEHUB_API_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = CTransformers(\n",
    "#                     model = \"TheBloke/Llama-2-7B-Chat-GGML\",\n",
    "#                     model_type=\"llama\",\n",
    "#                     max_new_tokens = 512,\n",
    "#                     temperature = 0.5\n",
    "#                     )\n",
    "\n",
    "llm_flan_ul2 = HuggingFaceHub(\n",
    "                            repo_id=\"google/flan-ul2\", \n",
    "                            model_kwargs={\"temperature\":0.1,\n",
    "                                        \"max_new_tokens\":256}\n",
    "                            )\n",
    "\n",
    "llm_flan_t5 = HuggingFaceHub(\n",
    "                            repo_id=\"google/flan-t5-xl\",\n",
    "                            model_kwargs={\"temperature\":0 },\n",
    "                            huggingfacehub_api_token=credentials['HUGGINGFACEHUB_API_TOKEN']\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "                                llm=llm_flan_ul2, \n",
    "                                verbose=True, \n",
    "                                memory=memory\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there! I am Zuu\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hi there! I am Zuu.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi there! I am Zuu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there! I am Zuu\n",
      "AI: Hi there! I am Zuu.\n",
      "Human: Do you like to play football?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, I like to play football.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Do you like to play football?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there! I am Zuu\n",
      "AI: Hi there! I am Zuu.\n",
      "Human: Do you like to play football?\n",
      "AI: Yes, I like to play football.\n",
      "Human: What is your name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'My name is Zuu.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there! I am Zuu\n",
      "AI: Hi there! I am Zuu.\n",
      "Human: Do you like to play football?\n",
      "AI: Yes, I like to play football.\n",
      "Human: What is your name?\n",
      "AI: My name is Zuu.\n",
      "Human: Tell me something about football ?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Football is a game played between two teams of eleven players on a field with a ball.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Tell me something about football ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### but how we can count and track tokens ?\n",
    "     - you can AutoTokenizer from transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi there! I am Zuu\n",
      "AI: Hi there! I am Zuu.\n",
      "Human: Do you like to play football?\n",
      "AI: Yes, I like to play football.\n",
      "Human: What is your name?\n",
      "AI: My name is Zuu.\n",
      "Human: Tell me something about football ?\n",
      "AI: Football is a game played between two teams of eleven players on a field with a ball.\n"
     ]
    }
   ],
   "source": [
    "# catch memory\n",
    "memory_buffer = memory.buffer\n",
    "print(memory_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Human', ':', '▁Hi', '▁there', '!', '▁I', '▁am', '▁Zu', 'u', '▁AI', ':', '▁Hi', '▁there', '!', '▁I', '▁am', '▁Zu', 'u', '.', '▁Human', ':', '▁Do', '▁you', '▁like', '▁to', '▁play', '▁football', '?', '▁AI', ':', '▁Yes', ',', '▁I', '▁like', '▁to', '▁play', '▁football', '.', '▁Human', ':', '▁What', '▁is', '▁your', '▁name', '?', '▁AI', ':', '▁My', '▁name', '▁is', '▁Zu', 'u', '.', '▁Human', ':', '▁Tell', '▁me', '▁something', '▁about', '▁football', '▁', '?', '▁AI', ':', '▁Football', '▁is', '▁', 'a', '▁game', '▁played', '▁between', '▁two', '▁teams', '▁of', '▁eleven', '▁players', '▁on', '▁', 'a', '▁field', '▁with', '▁', 'a', '▁ball', '.']\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(memory_buffer))\n",
    "print(len(tokenizer.tokenize(memory_buffer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there! I am Zuu\n",
      "AI: Hi there! I am Zuu.\n",
      "Human: Do you like to play football?\n",
      "AI: Yes, I like to play football.\n",
      "Human: What is your name?\n",
      "AI: My name is Zuu.\n",
      "Human: Tell me something about football ?\n",
      "AI: Football is a game played between two teams of eleven players on a field with a ball.\n",
      "Human: Name few football teams?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Manchester United Football Club is a football club based in Old Trafford, Greater Manchester, England.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Name few football teams?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi there! I am Zuu\n",
      "AI: Hi there! I am Zuu.\n",
      "Human: Do you like to play football?\n",
      "AI: Yes, I like to play football.\n",
      "Human: What is your name?\n",
      "AI: My name is Zuu.\n",
      "Human: Tell me something about football ?\n",
      "AI: Football is a game played between two teams of eleven players on a field with a ball.\n",
      "Human: Name few football teams?\n",
      "AI: The Manchester United Football Club is a football club based in Old Trafford, Greater Manchester, England.\n"
     ]
    }
   ],
   "source": [
    "memory_buffer = memory.buffer\n",
    "print(memory_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Human', ':', '▁Hi', '▁there', '!', '▁I', '▁am', '▁Zu', 'u', '▁AI', ':', '▁Hi', '▁there', '!', '▁I', '▁am', '▁Zu', 'u', '.', '▁Human', ':', '▁Do', '▁you', '▁like', '▁to', '▁play', '▁football', '?', '▁AI', ':', '▁Yes', ',', '▁I', '▁like', '▁to', '▁play', '▁football', '.', '▁Human', ':', '▁What', '▁is', '▁your', '▁name', '?', '▁AI', ':', '▁My', '▁name', '▁is', '▁Zu', 'u', '.', '▁Human', ':', '▁Tell', '▁me', '▁something', '▁about', '▁football', '▁', '?', '▁AI', ':', '▁Football', '▁is', '▁', 'a', '▁game', '▁played', '▁between', '▁two', '▁teams', '▁of', '▁eleven', '▁players', '▁on', '▁', 'a', '▁field', '▁with', '▁', 'a', '▁ball', '.', '▁Human', ':', '▁Name', '▁few', '▁football', '▁teams', '?', '▁AI', ':', '▁The', '▁Manchester', '▁United', '▁Football', '▁Club', '▁is', '▁', 'a', '▁football', '▁club', '▁', 'based', '▁in', '▁Old', '▁Tra', 'f', 'ford', ',', '▁Greater', '▁Manchester', ',', '▁England', '.']\n",
      "117\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(memory_buffer))\n",
    "print(len(tokenizer.tokenize(memory_buffer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### but this token count only measure the histrory / memory of the model. what if somehow the current user query exceed the token limit ?\n",
    "     - we can add the next query to prompt itself and continue the conversation\n",
    "     - for most of the cases, this will work fine. but what if the user query is too long ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there! I am Zuu\n",
      "AI: Hi there! I am Zuu.\n",
      "Human: Do you like to play football?\n",
      "AI: Yes, I like to play football.\n",
      "Human: What is your name?\n",
      "AI: My name is Zuu.\n",
      "Human: Tell me something about football ?\n",
      "AI: Football is a game played between two teams of eleven players on a field with a ball.\n",
      "Human: Name few football teams?\n",
      "AI: The Manchester United Football Club is a football club based in Old Trafford, Greater Manchester, England.\n",
      "Human: the next input\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt = conversation.prompt.format(input='the next input',history=memory.buffer)\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we added the user query to prompt itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_to_llm(chat_llm):\n",
    "    conversation_total_tokens = 0\n",
    "    new_conversation = ConversationChain(\n",
    "                                        llm=chat_llm, \n",
    "                                        verbose=False, \n",
    "                                        memory=ConversationBufferMemory()\n",
    "                                        )\n",
    "    \n",
    "    while True:\n",
    "        message = input(\"Human: \")\n",
    "        message_cp = message\n",
    "        if message=='exit':\n",
    "            print(f\"{conversation_total_tokens} tokens used in total in this conversation\")\n",
    "            break\n",
    "        if message:\n",
    "            formatted_prompt = conversation.prompt.format(input=message,history=new_conversation.memory.buffer) # here we do the trick as add current input to the history\n",
    "            num_tokens = len(tokenizer.tokenize(formatted_prompt))\n",
    "            conversation_total_tokens += num_tokens\n",
    "            print(f'tokens sent {num_tokens}')\n",
    "            response = new_conversation.predict(input=message)\n",
    "            response_num_tokens = len(tokenizer.tokenize(response))\n",
    "            conversation_total_tokens += response_num_tokens\n",
    "            print(f\"HUMAN: {message_cp}\")\n",
    "            print(f\"LLM: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens sent 64\n",
      "HUMAN: hi, there !\n",
      "LLM: hi, there!\n",
      "tokens sent 78\n",
      "HUMAN: what is your name ?\n",
      "LLM: my name is sarah.\n",
      "tokens sent 102\n",
      "HUMAN: hi sarah, are you like football ?\n",
      "LLM: i like football.\n",
      "tokens sent 117\n",
      "HUMAN: name few football teams ?\n",
      "LLM: i dont know\n",
      "382 tokens used in total in this conversation\n"
     ]
    }
   ],
   "source": [
    "chat_to_llm(llm_flan_ul2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
